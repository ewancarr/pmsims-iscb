@misc{dayimu2023,
  title = {Sample Size Determination via Learning-Type Curves},
  author = {Dayimu, Alimu and Simidjievski, Nikola and Demiris, Nikolaos and Abraham, Jean},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09575},
  eprint = {2303.09575},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2023-08-16},
  abstract = {This paper is concerned with sample size determination methodology for prediction models. We propose combining the individual calculations via a learning-type curve. We suggest two distinct ways of doing so, a deterministic skeleton of a learning curve and a Gaussian process centred upon its deterministic counterpart. We employ several learning algorithms for modelling the primary endpoint and distinct measures for trial efficacy. We find that the performance may vary with the sample size, but borrowing information across sample size universally improves the performance of such calculations. The Gaussian process-based learning curve appears more robust and statistically efficient, while computational efficiency is comparable. We suggest that anchoring against historical evidence when extrapolating sample sizes should be adopted when such data are available. The methods are illustrated on binary and survival endpoints.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {/Users/ewan/Zotero/storage/632D2AMU/Dayimu et al. - 2023 - Sample size determination via learning-type curves.pdf;/Users/ewan/Zotero/storage/5F3DDH5L/2303.html}
}

@article{dhiman2022,
  title = {Methodological Conduct of Prognostic Prediction Models Developed Using Machine Learning in Oncology: A Systematic Review},
  shorttitle = {Methodological Conduct of Prognostic Prediction Models Developed Using Machine Learning in Oncology},
  author = {Dhiman, Paula and Ma, Jie and Andaur Navarro, Constanza L. and Speich, Benjamin and Bullock, Garrett and Damen, Johanna A. A. and Hooft, Lotty and Kirtley, Shona and Riley, Richard D. and Van Calster, Ben and Moons, Karel G. M. and Collins, Gary S.},
  year = {2022},
  month = apr,
  journal = {BMC Medical Research Methodology},
  volume = {22},
  number = {1},
  pages = {101},
  issn = {1471-2288},
  doi = {10.1186/s12874-022-01577-x},
  urldate = {2023-07-31},
  abstract = {Describe and evaluate the methodological conduct of prognostic prediction models developed using machine learning methods in oncology.},
  keywords = {Machine learning,Methodology,Prediction},
  file = {/Users/ewan/Zotero/storage/WPZSQ4T9/Dhiman et al. - 2022 - Methodological conduct of prognostic prediction mo.pdf}
}

@article{figueroa2012,
  title = {Predicting Sample Size Required for Classification Performance},
  author = {Figueroa, Rosa L. and {Zeng-Treitler}, Qing and Kandula, Sasikiran and Ngo, Long H.},
  year = {2012},
  month = feb,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {12},
  number = {1},
  pages = {8},
  issn = {1472-6947},
  doi = {10.1186/1472-6947-12-8},
  urldate = {2023-08-17},
  abstract = {Supervised learning methods need annotated data in order to generate efficient models. Annotated data, however, is a relatively scarce resource and can be expensive to obtain. For both passive and active learning methods, there is a need to estimate the size of the annotated sample required to reach a performance target.},
  keywords = {Active Learning,Annotate Data,Learning Curve,Mean Absolute Error,Root Mean Square Error},
  file = {/Users/ewan/Zotero/storage/B3IIUESE/Figueroa et al. - 2012 - Predicting sample size required for classification.pdf;/Users/ewan/Zotero/storage/3WBRLJ69/1472-6947-12-8.html}
}

@article{meehan2022,
  title = {Clinical Prediction Models in Psychiatry: A Systematic Review of Two Decades of Progress and Challenges},
  shorttitle = {Clinical Prediction Models in Psychiatry},
  author = {Meehan, Alan J. and Lewis, Stephanie J. and Fazel, Seena and {Fusar-Poli}, Paolo and Steyerberg, Ewout W. and Stahl, Daniel and Danese, Andrea},
  year = {2022},
  month = jun,
  journal = {Molecular Psychiatry},
  volume = {27},
  number = {6},
  pages = {2700--2708},
  publisher = {{Nature Publishing Group}},
  issn = {1476-5578},
  doi = {10.1038/s41380-022-01528-4},
  urldate = {2023-07-31},
  abstract = {Recent years have seen the rapid proliferation of clinical prediction models aiming to support risk stratification and individualized care within psychiatry. Despite growing interest, attempts to synthesize current evidence in the nascent field of precision psychiatry have remained scarce. This systematic review therefore sought to summarize progress towards clinical implementation of prediction modeling for psychiatric outcomes. We searched MEDLINE, PubMed, Embase, and PsychINFO databases from inception to September 30, 2020, for English-language articles that developed and/or validated multivariable models to predict (at an individual level) onset, course, or treatment response for non-organic psychiatric disorders (PROSPERO: CRD42020216530). Individual prediction models were evaluated based on three key criteria: (i) mitigation of bias and overfitting; (ii) generalizability, and (iii) clinical utility. The Prediction model Risk Of Bias ASsessment Tool (PROBAST) was used to formally appraise each study's risk of bias. 228 studies detailing 308 prediction models were ultimately eligible for inclusion. 94.5\% of developed prediction models were deemed to be at high risk of bias, largely due to inadequate or inappropriate analytic decisions. Insufficient internal validation efforts (within the development sample) were also observed, while only one-fifth of models underwent external validation in an independent sample. Finally, our search identified just one published model whose potential utility in clinical practice was formally assessed. Our findings illustrated significant growth in precision psychiatry with promising progress towards real-world application. Nevertheless, these efforts have been inhibited by a preponderance of bias and overfitting, while the generalizability and clinical utility of many published models has yet to be formally established. Through improved methodological rigor during initial development, robust evaluations of reproducibility via independent validation, and evidence-based implementation frameworks, future research has the potential to generate risk prediction tools capable of enhancing clinical decision-making in psychiatric care.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Psychiatric disorders,Psychology},
  file = {/Users/ewan/Zotero/storage/PDD7B25P/Meehan et al. - 2022 - Clinical prediction models in psychiatry a system.pdf}
}

@article{navarro2021,
  title = {Risk of Bias in Studies on Prediction Models Developed Using Supervised Machine Learning Techniques: Systematic Review},
  shorttitle = {Risk of Bias in Studies on Prediction Models Developed Using Supervised Machine Learning Techniques},
  author = {Navarro, Constanza L. Andaur and Damen, Johanna A. A. and Takada, Toshihiko and Nijman, Steven W. J. and Dhiman, Paula and Ma, Jie and Collins, Gary S. and Bajpai, Ram and Riley, Richard D. and Moons, Karel G. M. and Hooft, Lotty},
  year = {2021},
  month = oct,
  journal = {BMJ},
  volume = {375},
  pages = {n2281},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {1756-1833},
  doi = {10.1136/bmj.n2281},
  urldate = {2023-07-31},
  abstract = {Objective To assess the methodological quality of studies on prediction models developed using machine learning techniques across all medical specialties. Design Systematic review. Data sources PubMed from 1 January 2018 to 31 December 2019. Eligibility criteria Articles reporting on the development, with or without external validation, of a multivariable prediction model (diagnostic or prognostic) developed using supervised machine learning for individualised predictions. No restrictions applied for study design, data source, or predicted patient related health outcomes. Review methods Methodological quality of the studies was determined and risk of bias evaluated using the prediction risk of bias assessment tool (PROBAST). This tool contains 21 signalling questions tailored to identify potential biases in four domains. Risk of bias was measured for each domain (participants, predictors, outcome, and analysis) and each study (overall). Results 152 studies were included: 58 (38\%) included a diagnostic prediction model and 94 (62\%) a prognostic prediction model. PROBAST was applied to 152 developed models and 19 external validations. Of these 171 analyses, 148 (87\%, 95\% confidence interval 81\% to 91\%) were rated at high risk of bias. The analysis domain was most frequently rated at high risk of bias. Of the 152 models, 85 (56\%, 48\% to 64\%) were developed with an inadequate number of events per candidate predictor, 62 handled missing data inadequately (41\%, 33\% to 49\%), and 59 assessed overfitting improperly (39\%, 31\% to 47\%). Most models used appropriate data sources to develop (73\%, 66\% to 79\%) and externally validate the machine learning based prediction models (74\%, 51\% to 88\%). Information about blinding of outcome and blinding of predictors was, however, absent in 60 (40\%, 32\% to 47\%) and 79 (52\%, 44\% to 60\%) of the developed models, respectively. Conclusion Most studies on machine learning based prediction models show poor methodological quality and are at high risk of bias. Factors contributing to risk of bias include small study size, poor handling of missing data, and failure to deal with overfitting. Efforts to improve the design, conduct, reporting, and validation of such studies are necessary to boost the application of machine learning based prediction models in clinical practice. Systematic review registration PROSPERO CRD42019161764.},
  chapter = {Research},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {34670780},
  file = {/Users/ewan/Zotero/storage/I6UMSBFH/Navarro et al. - 2021 - Risk of bias in studies on prediction models devel.pdf}
}

@article{riley2021,
  title = {Penalization and Shrinkage Methods Produced Unreliable Clinical Prediction Models Especially When Sample Size Was Small},
  author = {Riley, Richard D. and Snell, Kym I. E. and Martin, Glen P. and Whittle, Rebecca and Archer, Lucinda and Sperrin, Matthew and Collins, Gary S.},
  year = {2021},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {132},
  pages = {88--96},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2020.12.005},
  abstract = {OBJECTIVES: When developing a clinical prediction model, penalization techniques are recommended to address overfitting, as they shrink predictor effect estimates toward the null and reduce mean-square prediction error in new individuals. However, shrinkage and penalty terms ('tuning parameters') are estimated with uncertainty from the development data set. We examined the magnitude of this uncertainty and the subsequent impact on prediction model performance. STUDY DESIGN AND SETTING: This study comprises applied examples and a simulation study of the following methods: uniform shrinkage (estimated via a closed-form solution or bootstrapping), ridge regression, the lasso, and elastic net. RESULTS: In a particular model development data set, penalization methods can be unreliable because tuning parameters are estimated with large uncertainty. This is of most concern when development data sets have a small effective sample size and the model's Cox-Snell R2 is low. The problem can lead to considerable miscalibration of model predictions in new individuals. CONCLUSION: Penalization methods are not a 'carte blanche'; they do not guarantee a reliable prediction model is developed. They are more unreliable when needed most (i.e., when overfitting may be large). We recommend they are best applied with large effective sample sizes, as identified from recent sample size calculations that aim to minimize the potential for model overfitting and precisely estimate key parameters.},
  langid = {english},
  pmcid = {PMC8026952},
  pmid = {33307188},
  keywords = {Bias,Epidemiologic Research Design,Humans,{Models, Statistical},Overfitting,Penalization,Prognosis,Reproducibility of Results,Risk prediction models,Sample size,Sample Size,Shrinkage,Uncertainty},
  file = {/Users/ewan/Zotero/storage/FP6ULKFW/Riley et al. - 2021 - Penalization and shrinkage methods produced unreli.pdf}
}

@article{vanhouwelingen1990a,
  title = {Predictive Value of Statistical Models},
  author = {Van Houwelingen, J. C. and Le Cessie, S.},
  year = {1990},
  month = nov,
  journal = {Statistics in Medicine},
  volume = {9},
  number = {11},
  pages = {1303--1325},
  issn = {0277-6715},
  doi = {10.1002/sim.4780091109},
  abstract = {A review is given of different ways of estimating the error rate of a prediction rule based on a statistical model. A distinction is drawn between apparent, optimum and actual error rates. Moreover it is shown how cross-validation can be used to obtain an adjusted predictor with smaller error rate. A detailed discussion is given for ordinary least squares, logistic regression and Cox regression in survival analysis. Finally, the splitsample approach is discussed and demonstrated on two data sets.},
  langid = {english},
  pmid = {2277880},
  keywords = {Calibration,Female,Graft Survival,Humans,Kidney Transplantation,Least-Squares Analysis,{Models, Statistical},Ovarian Neoplasms,Prognosis,Regression Analysis,Survival Analysis}
}

@article{vansmeden2016,
  title = {No Rationale for 1 Variable per 10 Events Criterion for Binary Logistic Regression Analysis},
  author = {{van Smeden}, Maarten and {de Groot}, Joris A. H. and Moons, Karel G. M. and Collins, Gary S. and Altman, Douglas G. and Eijkemans, Marinus J. C. and Reitsma, Johannes B.},
  year = {2016},
  month = nov,
  journal = {BMC Medical Research Methodology},
  volume = {16},
  number = {1},
  pages = {163},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0267-3},
  urldate = {2023-07-31},
  abstract = {Ten events per variable (EPV) is a widely advocated minimal criterion for sample size considerations in logistic regression analysis. Of three previous simulation studies that examined this minimal EPV criterion only one supports the use of a minimum of 10 EPV. In this paper, we examine the reasons for substantial differences between these extensive simulation studies.},
  keywords = {Bias,EPV,Logistic regression,Sample size,Separation,Simulations},
  file = {/Users/ewan/Zotero/storage/LMP9GXS2/van Smeden et al. - 2016 - No rationale for 1 variable per 10 events criterio.pdf;/Users/ewan/Zotero/storage/NYSG7YFW/s12874-016-0267-3.html}
}

@article{wynants2020,
  title = {Prediction Models for Diagnosis and Prognosis of Covid-19: Systematic Review and Critical Appraisal},
  shorttitle = {Prediction Models for Diagnosis and Prognosis of Covid-19},
  author = {Wynants, Laure and Calster, Ben Van and Collins, Gary S. and Riley, Richard D. and Heinze, Georg and Schuit, Ewoud and Albu, Elena and Arshi, Banafsheh and Bellou, Vanesa and Bonten, Marc M. J. and Dahly, Darren L. and Damen, Johanna A. and Debray, Thomas P. A. and de Jong, Valentijn M. T. and Vos, Maarten De and Dhiman, Paula and Ensor, Joie and Gao, Shan and Haller, Maria C. and Harhay, Michael O. and Henckaerts, Liesbet and Heus, Pauline and Hoogland, Jeroen and Hudda, Mohammed and Jenniskens, Kevin and Kammer, Michael and Kreuzberger, Nina and Lohmann, Anna and Levis, Brooke and Luijken, Kim and Ma, Jie and Martin, Glen P. and McLernon, David J. and Navarro, Constanza L. Andaur and Reitsma, Johannes B. and Sergeant, Jamie C. and Shi, Chunhu and Skoetz, Nicole and Smits, Luc J. M. and Snell, Kym I. E. and Sperrin, Matthew and Spijker, Ren{\'e} and Steyerberg, Ewout W. and Takada, Toshihiko and Tzoulaki, Ioanna and van Kuijk, Sander M. J. and van Bussel, Bas C. T. and van der Horst, Iwan C. C. and Reeve, Kelly and van Royen, Florien S. and Verbakel, Jan Y. and Wallisch, Christine and Wilkinson, Jack and Wolff, Robert and Hooft, Lotty and Moons, Karel G. M. and van Smeden, Maarten},
  year = {2020},
  month = apr,
  journal = {BMJ},
  volume = {369},
  pages = {m1328},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {1756-1833},
  doi = {10.1136/bmj.m1328},
  urldate = {2023-08-16},
  abstract = {Objective To review and appraise the validity and usefulness of published and preprint reports of prediction models for prognosis of patients with covid-19, and for detecting people in the general population at increased risk of covid-19 infection or being admitted to hospital or dying with the disease. Design Living systematic review and critical appraisal by the covid-PRECISE (Precise Risk Estimation to optimise covid-19 Care for Infected or Suspected patients in diverse sEttings) group. Data sources PubMed and Embase through Ovid, up to 17 February 2021, supplemented with arXiv, medRxiv, and bioRxiv up to 5 May 2020. Study selection Studies that developed or validated a multivariable covid-19 related prediction model. Data extraction At least two authors independently extracted data using the CHARMS (critical appraisal and data extraction for systematic reviews of prediction modelling studies) checklist; risk of bias was assessed using PROBAST (prediction model risk of bias assessment tool). Results 126 978 titles were screened, and 412 studies describing 731 new prediction models or validations were included. Of these 731, 125 were diagnostic models (including 75 based on medical imaging) and the remaining 606 were prognostic models for either identifying those at risk of covid-19 in the general population (13 models) or predicting diverse outcomes in those individuals with confirmed covid-19 (593 models). Owing to the widespread availability of diagnostic testing capacity after the summer of 2020, this living review has now focused on the prognostic models. Of these, 29 had low risk of bias, 32 had unclear risk of bias, and 545 had high risk of bias. The most common causes for high risk of bias were inadequate sample sizes (n=408, 67\%) and inappropriate or incomplete evaluation of model performance (n=338, 56\%). 381 models were newly developed, and 225 were external validations of existing models. The reported C indexes varied between 0.77 and 0.93 in development studies with low risk of bias, and between 0.56 and 0.78 in external validations with low risk of bias. The Qcovid models, the PRIEST score, Carr's model, the ISARIC4C Deterioration model, and the Xie model showed adequate predictive performance in studies at low risk of bias. Details on all reviewed models are publicly available at https://www.covprecise.org/. Conclusion Prediction models for covid-19 entered the academic literature to support medical decision making at unprecedented speed and in large numbers. Most published prediction model studies were poorly reported and at high risk of bias such that their reported predictive performances are probably optimistic. Models with low risk of bias should be validated before clinical implementation, preferably through collaborative efforts to also allow an investigation of the heterogeneity in their performance across various populations and settings. Methodological guidance, as provided in this paper, should be followed because unreliable predictions could cause more harm than benefit in guiding clinical decisions. Finally, prediction modellers should adhere to the TRIPOD (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) reporting guideline. Systematic review registration Protocol https://osf.io/ehc47/, registration https://osf.io/wy245. Readers' note This article is the final version of a living systematic review that has been updated over the past two years to reflect emerging evidence. This version is update 4 of the original article published on 7 April 2020 (BMJ 2020;369:m1328). Previous updates can be found as data supplements (https://www.bmj.com/content/369/bmj.m1328/related\#datasupp). When citing this paper please consider adding the update number and date of access for clarity.},
  chapter = {Research},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {32265220},
  file = {/Users/ewan/Zotero/storage/HT38RX2F/Wynants et al. - 2020 - Prediction models for diagnosis and prognosis of c.pdf}
}

@misc{zimmer2022,
  title = {Simulation-Based {{Design Optimization}} for {{Statistical Power}}: {{Utilizing Machine Learning}}},
  shorttitle = {Simulation-Based {{Design Optimization}} for {{Statistical Power}}},
  author = {Zimmer, Felix and Debelak, Rudolf},
  year = {2022},
  month = mar,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/tnhb2},
  urldate = {2023-03-31},
  abstract = {The planning of adequately powered research designs increasingly goes beyond determining a suitable sample size. More challenging scenarios demand simultaneous tuning of multiple design parameter dimensions and can only be addressed using Monte Carlo simulation if no analytical approach is available. In addition, cost considerations, e.g., in terms of monetary costs, are a relevant target for optimization. In this context, optimal design parameters can imply a desired level of power at minimum cost or maximum power at a cost threshold. We introduce a surrogate modeling framework based on machine learning predictions to solve these optimization tasks. In a simulation study, we demonstrate the efficiency for a wide range of hypothesis testing scenarios with single- and multidimensional design parameters, including t-tests, ANOVA, item response theory models, and multilevel models. Our implementation is publicly available in an R package.},
  langid = {american},
  keywords = {machine learning,power analysis,Quantitative Methods,sample size,simulation,Social and Behavioral Sciences},
  file = {/Users/ewan/Sync/Work/Literature/Journal articles/PsyArXiv/2022/Zimmer_Debelak_2022_Simulation-based Design Optimization for Statistical Power.pdf}
}
