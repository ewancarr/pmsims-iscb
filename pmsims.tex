\documentclass[11pt]{beamer}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tcolorbox}
\usepackage[export]{adjustbox}
\usepackage{tikz}
\usepackage{minted}
\usemintedstyle{gruvbox-light}
\newcommand{\R}[1]{\mintinline[escapeinside=||]{r}{#1}}
\usepackage{mathrsfs}

\newcommand*{\colitem}[2]{%
    \item[\textcolor{#1}{\textbullet}] \textcolor{#1}{#2}
}

\newcommand{\sgap}{\vspace{0.5em}}
\newcommand{\bgap}{\vspace{0.8em}}

\newcommand\Wider[2][3em]{%
\makebox[\linewidth][c]{%
  \begin{minipage}{\dimexpr\textwidth+#1\relax}
  \raggedright#2
  \end{minipage}%
  }%
}

% Fonts --------------------------------------------------------------------------

\usepackage[scaled]{helvet}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[scale=0.95]{inconsolata}

% References ------------------------------------------------------------------

\usepackage[%
  backend     = biber,
  style       = numeric,
  sorting     = ynt,
  sortcites   = true,
  autocite    = superscript
]{biblatex}

\addbibresource{references.bib}

% tcolorbox -------------------------------------------------------------------

\newtcolorbox{cbox}[3][]
{
  colframe = #2!5,
  colback  = #2!5,
  coltitle = #2!50!black,  
  colbacktitle = #2!5,
  lefttitle = 1mm,
  toptitle = 2mm,
  bottomtitle = 0mm,
  colupper = #2!50!black,  
  title    = {#3},
  fonttitle = \bfseries\large,
  #1,
  arc=0.5mm,
  left=2.5mm,
  before upper={\setbeamercolor{item}{fg=#2!50!black}},
  after upper={\setbeamercolor{item}{fg=#2!50!black}}
}

\makeatletter
\def\beamer@origitem{%
  \@inmatherr\item\@ifnextchar[\@item{\@noitemargtrue\@item[\@itemlabel]%
  \csname beamer@thcfg@\beameritemnestingprefix item\endcsname% Insert colour in \beamer@thc@fg
  \ifx\beamer@thc@fg\@empty\relax\else\color{\beamer@thc@fg}\fi% Execute colour
  }}
\makeatother

\usepackage{graphicx}
\newcommand*{\img}[1]{%
    \raisebox{-.3\baselineskip}{%
        \includegraphics[
        height=\baselineskip,
        width=\baselineskip,
        keepaspectratio,
        ]{#1}%
    }%
}

\newcommand*{\timg}[2]{%
	\raisebox{-.5#1}{%
		\includegraphics[
			width=#1
		]{#2}%
    }%
}

\title[The pmsims package for R]{
    A simulation approach to calculating minimum sample sizes for prediction
    modelling
}
\subtitle{The pmsims package for R}
\date{29$^{\text{th}}$ August 2023}
\author[Biostatistics \& Health Informatics, KCL]{%
	Ewan Carr, Gordon Forbes, Diana Shamsutdinova, Daniel Stahl,
	and Felix Zimmer}
\institute[]{Department of Biostatistics \& Health Informatics\\ King's College London}
\titlegraphic{\includegraphics[height=1.5cm]{figures/kcl.png}}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

% Theme
\definecolor{KCLpurple}{RGB}{80, 20, 145}
\definecolor{KCLred}{RGB}{226, 35, 26}
\definecolor{KCLhotpink}{RGB}{200, 50, 150}
\definecolor{KCLpurple}{RGB}{80, 20, 145}
\definecolor{KCLseablue}{RGB}{0, 90, 210}
\definecolor{KCLtealblue}{RGB}{0, 154, 166}
\definecolor{KCLpeagreen}{RGB}{0, 181, 136}
\definecolor{KCLpantone}{RGB}{0, 35, 149}
\usecolortheme[named=KCLseablue]{structure}
\setbeamercolor{alerted text}{fg=KCLhotpink}

\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

\newenvironment{itemize*}%
{\begin{itemize}%
		\setlength{\itemsep}{100pt}%
		\setlength{\parskip}{0pt}}%
		{\end{itemize}}

\begin{document}

\maketitle

\section{Introduction}

\begin{frame}[c]{30-second version}
	\Large
	\begin{enumerate}
		\setlength{\itemsep}{12pt}

		\item Most prediction models use small samples.

		\item Small samples cause overfitting and imprecise estimates.

		\item Existing tools can estimate minimum samples for continuous,
		      binary, and survival outcomes.

		\item Nothing exists for other models or data types.

	\end{enumerate}

	\sgap

	\begin{cbox}{KCLtealblue}{}
		We're developing a simulation-based approach that works with any
		outcome or method.
	\end{cbox}
	\vspace{-1em}
\end{frame}

\begin{frame}[c]
	\includegraphics[width=\textwidth]{figures/group_photos.pdf}
\end{frame}

\begin{frame}[t]{This talk}
    \sgap
	\Large
	\begin{enumerate}
		\item Background
		      \begin{itemize}
			      \item What's the problem we're trying to solve?
			      \item What solutions currently exist?
		      \end{itemize}
		\item Our simulation-based approach
		      \begin{itemize}
			      \item Workflow and user interface
			      \item How it compares to other packages
		      \end{itemize}
		\item Demonstration
		\item Development status and next steps
	\end{enumerate}

    \onslide<2->
    \normalsize
	\vspace{6mm}
	\begin{cbox}{KCLtealblue}{}
		\centering
		\includegraphics[width=4em,valign=c]{figures/construction-site.pdf}
		\hspace{2em} {\large Under construction; feedback welcome.}
	\end{cbox}

\end{frame}

\section{Background}

\begin{frame}[t]{Most models are developed with inadequate samples}

	% Say: Prediction models can inform treatment decisions, facilitate
	% screening, and enable stratified care.

    % risk stratification, informing, shared decision making

	% Hundreds of prediction models are developed each year. Most have
	% inadequate samples.

	\begin{cbox}{KCLseablue}{}
		\large
		\begin{itemize}
			\item Inadequate sample sizes was the most common cause of bias
			      in 731 models for COVID-19.\autocite{wynants2020}
			\item Inadequate samples have been found in: \vspace{0.5em}
		\end{itemize}
		\begin{tabular}{rl}
			{\huge \alert{67\%}} & models for COVID-19\autocite{wynants2020}                      \\[0.5em]
			{\huge \alert{56\%}} & models using supervised machine learning\autocite{navarro2021} \\[0.5em]
			{\huge \alert{73\%}} & models in psychiatry\autocite{meehan2022}                      \\[0.3em]
		\end{tabular}
		% \begin{itemize}
		% 	\item Just \alert{8\%} of machine learning models in oncology
		% 	      reported a sample size justification\autocite{dhiman2022}.
		% \end{itemize}
	\end{cbox}

Inadequate samples = research waste

	\begin{itemize}
		\item Inadequate samples lead to overfitting and inaccurate estimates
		      of model parameters.

		      % Overfitting is where the model captures idiosyncrasies
		      % of the development sample, producing inflated estimates of predictive
		      % performance that cannot be replicated in the target population.

		\item This may generate inappropriate decisions about patient care or
		      lead to models not being implemented into clinical practice.

		\item Data collection can be invasive and inconvenient and diverts
		      resources from other activities that benefit patients.

	\end{itemize}

	\begin{cbox}{KCLpurple}{}
		\large
		Ensuring sample sizes are sufficient \textbf{before model development}
		would improve patient outcomes by avoiding models developed with
		inadequate samples and reducing participant burden.
	\end{cbox}

\end{frame}

\begin{frame}[t]{What tools exist?}
	\centering
	\begin{columns}
		\begin{column}[c]{0.6\textwidth}
			\centering
			Most studies ignore sample size.
			\bgap%

			Or use rules of thumb (e.g., 10 events per variable) that have no
			rationale in prediction modelling\autocite{vansmeden2016}.
		\end{column}
		\begin{column}[c]{0.4\textwidth}
			\includegraphics[width=\textwidth]{figures/bury-head.png}%
		\end{column}
	\end{columns}

	% This rule of thumb has been shown to have no rationale, especially
	% in prediction model research, as its evidence base is mainly informed by
	% simulation studies that investigate the performance of estimating
	% covariate-outcome relationships.
	% https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-023-02008-1
    \bgap

	\begin{columns}
		\begin{column}[c]{0.5\textwidth}
			\includegraphics[width=\textwidth]{figures/riley2.png}%
		\end{column}
		\begin{column}[c]{0.5\textwidth}
            In 2018, Riley et al.\ released \alert{pmsampsize} for R and Stata.
		\end{column}
	\end{columns}

\end{frame}

\begin{frame}[c]{We \img{figures/heart.png} pmsampsize, but\ldots}

	\large
	We increasingly need to estimate minimum samples for:\ \sgap
	\begin{itemize}
		\colitem{KCLpurple}{\textbf{Other models} (e.g., machine learning algorithms, random
			forests, gradient boosting);}
		\colitem{KCLseablue}{\textbf{Other data types} (e.g., longitudinal, clustered)}
	\end{itemize}

    \vspace{0.5em}
    We're creating a \textbf{simulation-based framework} to estimate sample
    sizes for prediction.
    \vspace{0.5em}

	\begin{cbox}{KCLhotpink}{The pmsims package for R}
		\begin{tabular}{rp{0.8\textwidth}}
			\textbf{Flexible}      & Any model or data type             \\
			\textbf{User-friendly} & Defaults for common scenarios      \\
			\textbf{Efficent}      & Estimation via surrogate modelling
		\end{tabular}
	\end{cbox}

\end{frame}

\begin{frame}[c]{Our approach}
    \begin{cbox}[bottom=4mm]{KCLpeagreen}{Setting}
		\begin{enumerate}
			\setlength{\itemsep}{7pt}
            \item A \textbf{study population} represented by outcome-related individual
			      characteristics (i.e., candidate predictors).
              \item A chosen statistical or machine learning \textbf{model}.
              \item Expected achievable \textbf{large-sample performance}, $P^{*}$, given population and model.
              \item Minimum \textbf{acceptable test performance} of the model, $P^{OK}$.
		\end{enumerate}
	\end{cbox}

	\onslide<2->
	\sgap
	\centering
	\includegraphics[height=3.6em]{figures/target.pdf}
	\hspace{0.7em}
	\large
	\parbox[b]{0.8\textwidth}{ \raggedright \textcolor{KCLpantone}{ Find the
	    minimum sample that ensures test performance of $P^{OK}$ with
	    probability of 80\%, given the population, predictors, and $P^{*}$. }
	    }

	% i.e., drawing a random sample of size $n$ will result in a model with
	% performance $>P^{OK}$ in more than 80\% of cases.

\end{frame}

\begin{frame}{How does it work?}

	\begin{columns}[T]
		\begin{column}{0.4\textwidth}
			\centering
			\includegraphics<+>[width=0.85\textwidth]{figures/workflow-a.pdf}%
			\includegraphics<+>[width=0.85\textwidth]{figures/workflow-b.pdf}%
			\includegraphics<+>[width=0.85\textwidth]{figures/workflow-c.pdf}%
			\includegraphics<+>[width=0.85\textwidth]{figures/workflow-d.pdf}%
		\end{column}
		\begin{column}{0.7\textwidth}
            \large
			\only<1>{
				\vspace{1em}
				The user specifies:
				\begin{enumerate}
					\item The candidate predictors (number, type)
					\item The chosen statistical model
					\item The expected large sample performance ($P^{*}$)
					\item The minimum acceptable performance ($P^{OK}$)
				\end{enumerate}
			}%
			\only<2>{
				\vspace{5.5em}
				Based on their input, we set:
				\begin{enumerate}
					\item A data-generating function
					\item A model function
					\item A metric function
				\end{enumerate}
			}%
			\only<3>{
				\vspace{10em}
				We tune the data generating model, so the large sample
				performance is $P^{*}$.
			}
		\end{column}
	\end{columns}

\end{frame}

\begin{frame}[t]{Performing the search}
	\tikz[overlay, remember picture]
	\node[%
		anchor=north east,
		yshift=0.2cm,
		xshift=-0.5cm]at (current page.north east) {
		\includegraphics[width=0.28\textwidth]{figures/tortoise.png}%
	};

	\vspace{-0.5em}
	An exhaustive grid search would be too slow.
	\Wider{%
		\vspace{2em}
		\begin{cbox}{KCLpurple}{Surrogate modelling with mlpwr}
			% Surrogate modeling aims to approximate a relationship that is costly to
			% investigate with a cheaper function.

			\begin{itemize}
				\item Approximates the relationship between sample size and
				      $P^{OK}$ using Gaussian process regression.
				\item Also referred to as `learning curve
				      fitting'\autocite{figueroa2012, dayimu2023}.
				\item Uses the mlpwr R package by Zimmer and
				      Debelak\autocite{Zimmer.inpress}.
			\end{itemize}
		\end{cbox}

		\vspace{0.3em}
		\centering
		\includegraphics[width=\textwidth]{figures/surrogate_algorithm.pdf}%
	}
\end{frame}

\begin{frame}[c]{}
	\Wider[2em]{
		\includegraphics<+>[width=\textwidth]{figures/surrogate-1.pdf}%
		\includegraphics<+>[width=\textwidth]{figures/surrogate-3.pdf}%
		\includegraphics<+>[width=\textwidth]{figures/surrogate-4.pdf}%
		\includegraphics<+>[width=\textwidth]{figures/surrogate-5.pdf}%
		\includegraphics<+>[width=\textwidth]{figures/surrogate-6.pdf}%
	}

\end{frame}

\begin{frame}[t]{What is the performance of a prediction model?}
	\Wider{
		\vspace{0.6em}

		{\centering
			\includegraphics[width=\textwidth]{figures/performance_flow.pdf}%
		}%

		\vspace{0.5em}%

		\textbf{Apparent vs. test performance (or ``actual'' performance)}

		\begin{itemize}
			\item Train/test performances are random variables of the drawn
			      sample.
			\item Test performance is expected to be worse than apparent; but
			      difference reduced with higher n.
			\item A prediction model is as good as its test performance.
		\end{itemize}

		\vspace{1em}
		\includegraphics[width=\textwidth]{figures/diana-auc.pdf}%
	}
\end{frame}

\begin{frame}[t]{How do we assess performance?}

	We identify the minimum sample that meets \underline{three} criteria:

	\begin{cbox}{KCLpeagreen}{1.\ Overall fit} \\
		Within 0.1 of the achievable large sample fit (e.g., $R^2$, Brier).
	\end{cbox}

	\begin{cbox}{KCLseablue}{2.\ Discrimination} \\
		Within 0.1 of the achievable large sample discrimination (e.g., C-statistic, AUC).
	\end{cbox}

	\begin{cbox}{KCLhotpink}{3.\ Calibration slope} \\
		A calibration slope of 0.9 to 1.1.
	\end{cbox}

	The threshold and metric are user-configurable.

\end{frame}


\begin{frame}[t]{Two approaches to estimating minimum samples}

	\footnotesize
	\Wider[5em]{
		\begin{tabular}{lp{0.4\textwidth}p{0.4\textwidth}}
             & {\Large \textcolor{KCLpurple}{pmsims}} & {\Large \textcolor{KCLhotpink}{pmsampsize}}    \\[4mm]
            Target? & Minimum development sample that ensures expected apparent and test performances are sufficiently close. &
			Minimum development sample that ensures test performance of $P^{OK}$ with 80\% probability. \\[5mm]
			How?                               & \textcolor{KCLpurple}{Simulate absolute test performance}
			& \textcolor{KCLhotpink}{Analytical closeness of test vs.\ train performance} \\
			&
			\begin{minipage}[t]{0.4\textwidth}
			\begin{itemize}
				\item Tune data generator to an expected achievable performance.
				\item For training data of different sizes, use mlpwr to find n at which
				      80\% of test performances achieves $P^{OK}$.
\item Calibration slope criterion is similar to uniform shrinkage criterion
      (slope is defined as minimizing the error between
				            $y^{test}$ and $\alpha + slope \times
				            \hat{y}^{test}$).
			\end{itemize}
			\end{minipage} &
			\begin{minipage}[t]{0.4\textwidth}
				\begin{itemize}
                    \item Targets small train-test difference in $R^2$; or uniform shrinkage above given threshold (e.g., 0.9).
			\item Uniform shrinkage:\ considers GLM models, where estimates depend on a
			      linear predictor, $x^{T}\hat{\beta}$, with
			      $\hat{\beta}-$ OLS/ML estimates from the training
			      sample.
			\item Using $s \cdot x^{T} \hat{\beta}$, instead of
			      $x^{T}\hat{\beta}$ may prevent overfitting and
			      perform better on unseen cases.
			\end{itemize}
			\end{minipage}
		\end{tabular}
	}
    FIX THE HATS

\end{frame}

\begin{frame}[t]{What are the distinctive features of these approaches?}
	\footnotesize
	\Wider[4.5em]{
		\begin{tabular}{@{}p{0.5\textwidth}@{}p{0.5\textwidth}}
			{\Large pmsims}                     & {\Large pmsampsize} \\
			\begin{minipage}[t]{0.45\textwidth}
				\begin{itemize}
					\item Targets absolute performance
					\item Flexible
					      % Machine learning, multilevel models, etc.
					      % heteroscedasticity and multicollinearity in the data
					\item Does not aim to prevent overfitting per se
					\item Targets test performance*
					\item Adjusts recommendations for test performance
					      variance**
				\end{itemize}
				However:\
				\begin{itemize}
					\item Computationally demanding
					\item User must specify data/model for complex designs
					\item Simulation variability
				\end{itemize}
			\end{minipage} &
			\begin{minipage}[t]{0.45\textwidth}
				\begin{itemize}
					\item Fast, closed-form solutions
					\item Ensures sufficient training sample to prevent
					      overfitting
				\end{itemize}
				However:\
				\begin{itemize}
					\item Closed-form only for some
					      models\autocite{riley2021}.
					\item Does not adjust predictions to the variance of the
					      test performance\autocite{vanhouwelingen1990a}.
					\item As only one training sample will be available to the
					      model developers, actual performance once deployed
					      may be much lower**.
				\end{itemize}
			\end{minipage}
		\end{tabular}
	}

\end{frame}

\begin{frame}[t]{}

	\Wider{
		Compared to pmsampsize, our approach will require:
		\vspace{-0.3em}
		\begin{tcolorbox}[arc=0.3mm, colback=white]
			\textcolor{KCLpeagreen}{Smaller N} for machine learning models:\
			\begin{itemize}
				\setlength\itemsep{0.2mm}
				\item[--] Tend to overfit but may still achieve high test
					performance.
				\item[--] Targeting shrinkage requires $\uparrow$N than performance.
			\end{itemize}
			\textcolor{KCLred}{Larger N} for noisy data and models with high variance:\
			\begin{itemize}
				\setlength\itemsep{0mm}
				\item[--] 0.2 quantile test performance < mean performance.
			\end{itemize}
		\end{tcolorbox}
		\centering
		\includegraphics[width=\textwidth]{figures/diana-slope.pdf}%
	}

\end{frame}

\begin{frame}[c,fragile]{The user interface}

	\vspace{1em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/interfaces.pdf}%
	\vspace{-1.5em}

	\begin{columns}
		\begin{column}[t]{0.55\textwidth}
			\begin{cbox}{gray}{}
				\begin{minted}[autogobble,fontsize=\small]{r}
simulate_continuous <- 
  function(
    signal_parameters = 30,
    noise_parameters = 0,
    min_sample_size = 300,
    max_sample_size = 10000,
    large_discrimination = 0.7,
    minimum_threshold = 0.1,
    model = "lm",
    metric = "r2",
    ...
  ) 
            \end{minted}
			\end{cbox}
		\end{column}
		\begin{column}[t]{0.55\textwidth}
			\begin{cbox}{gray}{}
				\begin{minted}[autogobble,fontsize=\small]{r}
simulate_binary <- 
  function(
    signal_parameters = 30,
    noise_parameters = 0,
    baseline_prob = 0.1,
    min_sample_size = 300,
    max_sample_size = 10000,
    large_discrimination = 0.8,
    minimum_threshold = 0.1,
    metric = "auc",
    model = "glm",
    ...
  )
            \end{minted}
			\end{cbox}
		\end{column}
	\end{columns}

\end{frame}

\begin{frame}[b,fragile]{Example 1:\ Binary outcome, logistic regression}
	\Wider{
		\includegraphics[width=0.9\textwidth]{figures/demo1.pdf}%
	}

	\tikz[overlay, remember picture]
	\node[anchor=north east, yshift=-0.9cm] at (current page.north east) {
		\begin{cbox}[width=4.8cm]{gray}{}%
			\begin{minted}[autogobble,fontsize=\scriptsize]{r}
simulate_binary(
  signal_parameters = 20,
  baseline_prob = 0.1,
  min_sample_size = 500,
  max_sample_size = 15000,
  large_discrimination = ...,
  minimum_threshold = 0.1,
  metric = "auc",
  model = "glm",
)
            \end{minted}
		\end{cbox}
	};

\end{frame}

\begin{frame}[b,fragile]{Example 2:\ Binary outcome, LASSO regression}

	\Wider{
		\includegraphics[width=0.9\textwidth]{figures/demo2.pdf}%
	}

	\tikz[overlay, remember picture]
	\node[anchor=north east, yshift=-0.9cm] at (current page.north east) {
		\begin{cbox}[width=4.8cm]{gray}{}%
			\begin{minted}[%
                highlightlines=9,
                autogobble,
                fontsize=\scriptsize]{r}
simulate_binary(
  signal_parameters = 20,
  baseline_prob = 0.1,
  min_sample_size = 500,
  max_sample_size = 15000,
  large_discrimination = ...,
  minimum_threshold = 0.1,
  metric = "auc",
  model = "lasso",
)
            \end{minted}
		\end{cbox}
	};

\end{frame}

\begin{frame}[c,fragile]{Example 3:\ Custom model function}

	What if a model hasn't been implemented?        \vspace{-0.5em}%%

	\begin{columns}
		\begin{column}[T]{0.5\textwidth}
			\begin{cbox}{gray}{}%
				\begin{minted}[autogobble,fontsize=\scriptsize]{r}
model_function <- function(d) {
  dmat <- xgboost::xgb.DMatrix(
    as.matrix(d[, -1]),
    label = d[, 1]
  )
  param <- list(
    objective = "binary:logistic",
    booster = "gblinear",
    alpha = 0.0001,
    lambda = 1
  )
  xgboost::xgb.train(
    param,
    dmat,
    nrounds = 2
  )
}
            \end{minted}
			\end{cbox}

		\end{column}
		\begin{column}[T]{0.55\textwidth}
			\begin{cbox}{gray}{}%
				\begin{minted}[%
                autogobble,
                fontsize=\scriptsize]{r}
metric_function <- function(data,
                            fit,
                            model) {
  dmat <- xgboost::xgb.DMatrix(
    as.matrix(data[, -1]), 
    label = data[, 1]
  )
  y_hat <- predict(fit, dmat)
  pROC::auc(data[, 1], y_hat)[1]
}
            \end{minted}
			\end{cbox}

			\begin{cbox}{gray}{}%
				\begin{minted}[%
                highlightlines=2-4,
                autogobble,
                fontsize=\scriptsize]{r}
simulate_custom(
  data_function = data_function,
  model_function = model_function,
  metric_function = metric_function,
  ...
)
            \end{minted}
			\end{cbox}
		\end{column}
	\end{columns}

\end{frame}

\section{Conclusion}

\begin{frame}[t]{Development status}

	We're still developing the package.

	% Say: it works, but more testing needed.

	\begin{cbox}[colframe=KCLhotpink!50!white]{KCLhotpink!30!white}{}

		\begin{tabular}{cp{0.8\textwidth}}
			\includegraphics[width=2em,valign=c]{figures/mastodon.pdf} &
			\href{https://fediscience.org/@ewan}{\textcolor{KCLhotpink}{fediscience.org/@ewan}}
			for updates                                                  \\[1.6em]
			\includegraphics[width=2em,valign=c]{figures/bell.pdf}     &
			Enter email at
			\href{https://tinyurl.com/is-pmsims-ready-yet}{\textcolor{KCLhotpink}{tinyurl.com/is-pmsims-ready-yet}}
			to get one email when a public release is available.         \\[1.6em]
			\includegraphics[width=2em,valign=c]{figures/hi.pdf}       &
			Come and talk to us.
		\end{tabular}
	\end{cbox}

	Next steps:\

	% \begin{enumerate}
	%     \item[\{KCLpurple}{Data and model generators for common machine
	%         learning algorithms}
	%     \colitem{KCLpurple}{\ldots for longitudinal and clustered data.}
	% \end{enumerate}

	\begin{cbox}{white}{Next steps}
		\begin{columns}
			\begin{column}[T]{0.45\textwidth}
				\begin{enumerate}
					\item[1.] \textcolor{KCLpurple}{Machine learning}
					\item[2.] \textcolor{KCLpurple}{Longitudinal data}
				\end{enumerate}
			\end{column}
			\begin{column}[T]{0.45\textwidth}
				\begin{enumerate}
					\item[3.] \textcolor{KCLpurple}{Common data types}
						% e.g., clinical, NLP, genetic.
					\item[4.] \textcolor{KCLpurple}{Performance}
				\end{enumerate}
			\end{column}
		\end{columns}
	\end{cbox}

	% Say: criteria/models are subject to change.

\end{frame}

\begin{frame}[t]{What's next?}
\begin{cbox}{KCLtealblue}{1.\ Machine learning}{}

\end{cbox}

\begin{cbox}{KCLpurple}{2.\ Longitudinal data}{}

\end{cbox}

\begin{cbox}{KCLseablue}{3.\ Common data types}{}
    e.g., clinical, NLP, genetic.

\end{cbox}

\begin{cbox}{KCLhotpink}{4.\ Performance}{}

\end{cbox}

\end{frame}

\begin{frame}[t]
	\centering
	Thank you for listening.
    \begin{tabular}{cl}
        \includegraphics[width=1.5em,valign=c]{figures/email.pdf} & \textcolor{KCLhotpink}{ewan.carr@kcl.ac.uk}\\[1.7em] 
        \includegraphics[width=1.5em,valign=c]{figures/mastodon.pdf} & \href{https://fediscience.org/@ewan}{\textcolor{KCLhotpink}{fediscience.org/@ewan}} 
    \end{tabular}
end{frame}

\appendix

\begin{frame}[allowframebreaks]{References}
	\renewcommand*{\bibfont}{\scriptsize}
	\printbibliography
\end{frame}

\end{document}
